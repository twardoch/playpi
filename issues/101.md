

Read @external/spec-cla.md @external/spec-gem.md @external/spec-gpt.md and into @PLAN.md /plan a very detailed plan on how to implement the `playpi` package in this folder. 

Make sure we use `hatch` and `hatch-vcs` (git-tag-based semver), and `uv` and `ruff`. 

Focus on lean, clean, performant code that uses the `playwrightauthor` package, and uses good modern techniques, and useful packages. 

Once youâ€™ve completed @PLAN.md , analyze and review and refine it. Then into @TODO.md write a flat itemized representation of the plan. 

## Some info

I need to use playwright in Python to run and then collect certain browser jobs (workflows on a certain website, like Google Deep Research) which may run for 5 or 10 minutes, and max 3 jobs of the given type can run in parallel, but once one given job of the type is finished, we could launch another one. 

**Problem Analysis**: You need a bounded worker pool that maintains exactly 3 concurrent browser jobs, with a queue of 50 tasks that auto-starts when slots open up.

**Constraints**: Browser memory usage, rate limits, max 3 parallel sessions.

**Solution**: You don't need heavy orchestration - just a simple async semaphore pattern.

## The Simplest Solution That Actually Works

```python
#!/usr/bin/env python3
# this_file: deep_research_runner.py

import asyncio
from playwright.async_api import async_playwright
from datetime import datetime
import json
from pathlib import Path

class ResearchRunner:
    def __init__(self, max_concurrent=3):
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.results = []
        
    async def run_single_research(self, research_id, query):
        """Run one deep research job with browser automation."""
        async with self.semaphore:  # This ensures only 3 run at once
            print(f"[{datetime.now():%H:%M:%S}] Starting research {research_id}: {query}")
            
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=False)  # Set True for production
                context = await browser.new_context()
                page = await context.new_page()
                
                try:
                    # Your actual automation logic here
                    await page.goto("https://deepmind.google/research/")
                    
                    # Simulate 5-10 minute job
                    # Replace with your actual workflow
                    await asyncio.sleep(5)  # Testing - use your real logic
                    
                    result = {
                        "id": research_id,
                        "query": query,
                        "status": "completed",
                        "timestamp": datetime.now().isoformat()
                    }
                    
                except Exception as e:
                    result = {
                        "id": research_id,
                        "query": query, 
                        "status": "failed",
                        "error": str(e),
                        "timestamp": datetime.now().isoformat()
                    }
                
                finally:
                    await browser.close()
                    
                print(f"[{datetime.now():%H:%M:%S}] Finished research {research_id}")
                return result
    
    async def run_all_research(self, queries):
        """Run all research queries with concurrent limit."""
        tasks = [
            self.run_single_research(i, query) 
            for i, query in enumerate(queries, 1)
        ]
        
        # This automatically manages the queue
        self.results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Save results
        Path("results.json").write_text(
            json.dumps(self.results, indent=2)
        )

async def main():
    # Your 50 research queries
    queries = [f"Deep research query {i}" for i in range(1, 51)]
    
    runner = ResearchRunner(max_concurrent=3)
    await runner.run_all_research(queries)
    
    print(f"\nCompleted {len(runner.results)} researches")

if __name__ == "__main__":
    asyncio.run(main())
```

## Why This Works Better Than Prefect/Dask

1. **Semaphore** handles the "max 3 concurrent" requirement automatically
2. **asyncio.gather()** manages the queue - starts new jobs as others finish
3. **No external dependencies** beyond Playwright
4. **Simple to debug** - it's just 60 lines of code

## Adding Robustness (If Needed)

```python
# Add retries
async def run_with_retry(self, func, max_retries=3):
    for attempt in range(max_retries):
        try:
            return await func()
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            await asyncio.sleep(2 ** attempt)  # Exponential backoff

# Add progress tracking
from tqdm.asyncio import tqdm

async def run_all_research_with_progress(self, queries):
    tasks = [...]
    results = []
    for task in tqdm.as_completed(tasks, total=len(tasks)):
        result = await task
        results.append(result)
```

## Only Add Complexity When You Need:

- **Scheduling**: Use `schedule` or `apscheduler`
- **Persistence**: Use `sqlite` for job status
- **Monitoring**: Use `rich` for live dashboard

But for 50 jobs with 3 concurrent browsers? The simple solution above is perfect.

**Test Strategy**: Run with 5 test queries first, monitor memory usage, then scale to 50.


## Extra

### References 

external/01in/crapi_core.py
external/01in/crapi_grok-3.py
external/01in/crapi_grok.py
external/01in/crapi_grok_cookies.pkl
external/01in/crapi_youcom.py
external/01in/crapi_youcom_cookies.pkl
external/01in/geminpy.txt
external/01in/playwright-google-deep-research.py
external/01in/playwrightauthor.txt
external/01in/test_chrome.py
external/01in/test_grok.py
external/01in/test_grok_extra.py
external/01in/test_nodriver.py
external/01in/virginia-clemm-poe.txt

### Analysis 

external/spec-cla.md
external/spec-gem.md
external/spec-gpt.md

### Skeleton

package.toml
pyproject.toml
src/
src/playpi/
src/playpi/__version__.py
src/playpi/playpi.py
tests/
tests/test_package.py
